# Task 1: Mid-Training Support - COMPLETE âœ…

**Date:** 2025-11-15  
**Status:** âœ… Implementation Complete

---

## ğŸ¯ What Was Implemented

Mid-training support for the miniGPT pipeline, allowing continuation from pretrained models with specialized data mixtures to enhance specific capabilities (code, reasoning, etc.) while preventing catastrophic forgetting through data replay.

---

## ğŸ“ Files Created/Modified

### 1. **New Config: `config/data/midtrain/phase1/default.yaml`**

A mid-training data mixture configuration with:
- **30% Replay**: TinyStories data to prevent forgetting
- **35% Code**: Python code instructions dataset
- **35% Reasoning**: MMLU math/reasoning dataset

**Dataset Sources (HuggingFace):**
- `roneneldan/TinyStories` - General text replay
- `iamtarun/python_code_instructions_18k_alpaca` - Code learning
- `lighteval/mmlu` - Math and reasoning

**New Filter:** `code_quality` filter to ensure code datasets actually contain code

### 2. **Enhanced: `dataset/filters.py`**

Added two new functions:

#### `filter_by_code_quality()`
Filters datasets to ensure they contain actual code by detecting:
- Indented lines (spaces/tabs)
- Programming keywords (def, class, import, function, etc.)
- Code symbols ({}, [], ==, !=, etc.)
- Semicolons and parentheses

#### `calculate_code_ratio()`
Calculates the ratio of code-like content (0.0 to 1.0) using heuristics.

### 3. **New Script: `trainer/train_midtrain.py`**

Complete mid-training script based on `train_pretrain.py` with:

**New Features:**
- âœ… `get_midtrain_lr()` - LR re-warmup schedule
- âœ… LR warmup from LR/10 â†’ LR (linear)
- âœ… LR decay from LR â†’ LR/10 (cosine)
- âœ… `--warmup_ratio` parameter (default 0.05)
- âœ… Validation check: requires `--from_weight` (cannot start from scratch)
- âœ… Updated docstrings and help text for mid-training context

**Key Differences from Pretrain:**
- Lower default LR (2e-4 vs 5e-4)
- Re-warmup enabled by default
- Must load from pretrained checkpoint
- Save prefix: `midtrain` instead of `pretrain`
- WandB project: `MiniMind-MidTrain`

---

## ğŸ”§ How It Works

### Learning Rate Schedule

```
Step 0:        LR/10  (start low to avoid destabilizing)
   â†“ warmup
Step W:        LR     (peak learning rate)
   â†“ decay
Step MAX:      LR/10  (end at minimum)

Where:
- W = warmup_ratio * total_steps (default 5%)
- LR = args.learning_rate
```

### Data Mixture Philosophy

1. **Replay (30%)**: Prevents catastrophic forgetting of general knowledge
2. **Specialization (70%)**: New skills (code, math, reasoning)
3. **Quality Filters**: Ensures high-quality samples only

---

## ğŸ“ Usage Commands

### Basic Mid-Training

```bash
python trainer/train_midtrain.py \
    --data_config config/data/midtrain/phase1/default.yaml \
    --from_weight pretrain \
    --epochs 1 \
    --batch_size 16 \
    --learning_rate 2e-4 \
    --warmup_ratio 0.05 \
    --device cuda:0
```

### With WandB Logging

```bash
python trainer/train_midtrain.py \
    --data_config config/data/midtrain/phase1/default.yaml \
    --from_weight pretrain \
    --use_prepared \
    --epochs 1 \
    --batch_size 16 \
    --learning_rate 2e-4 \
    --use_wandb \
    --wandb_project "miniGPT-test-midtrain"
```

### Resume from Checkpoint

```bash
python trainer/train_midtrain.py \
    --data_config config/data/midtrain/phase1/default.yaml \
    --from_resume 1 \
    --epochs 2
```

---

## ğŸ§ª Testing

### Step 1: Prepare Dataset

```bash
# Prepare the mid-training mixture
python scripts/prepare_dataset.py \
    --config config/data/midtrain/phase1/default.yaml \
    --output_dir dataset/

# This creates:
#   dataset/midtrain_phase1_default_train.jsonl
#   dataset/midtrain_phase1_default_val.jsonl
```

### Step 2: Test Training (Quick)

```bash
# Test with small batch for 100 steps
python trainer/train_midtrain.py \
    --data_config config/data/midtrain/phase1/default.yaml \
    --from_weight pretrain \
    --use_prepared \
    --epochs 1 \
    --batch_size 4 \
    --save_interval 100 \
    --log_interval 10 \
    --device cuda:0
```

### Step 3: Verify Output

Expected output should show:
```
âœ… Loaded pretrained model from: pretrain
Mid-training with LR re-warmup: 75 warmup steps out of 1485 total steps
LR schedule: 2.00e-05 â†’ 2.00e-04 (warmup) â†’ 2.00e-05 (decay)
...
Epoch:[1/1](10/1485) loss:X.XXXXXX lr:0.0000XXXXXX grad_norm:X.XX tokens/s:XXXXX MFU:X.XX% ...
```

---

## ğŸ” Verification Checklist

- [x] Config file created with 3 HuggingFace datasets
- [x] Mix ratios sum to 1.0
- [x] Code quality filter implemented
- [x] Filter works with existing apply_filters()
- [x] train_midtrain.py script created
- [x] LR re-warmup function added
- [x] LR calculation updated in train_epoch
- [x] Docstrings updated
- [x] Argument parser updated
- [x] from_weight validation added
- [x] WandB project name changed
- [x] Save weight prefix changed to 'midtrain'

---

## ğŸ“Š Dataset Details

### TinyStories Replay (30%)
- **Source**: `roneneldan/TinyStories`
- **Purpose**: Prevent forgetting of general language
- **Samples**: ~30,000
- **Filters**: length(100-2000), quality(0.3)

### Code Instructions (35%)
- **Source**: `iamtarun/python_code_instructions_18k_alpaca`
- **Purpose**: Learn programming patterns
- **Samples**: ~35,000
- **Filters**: length(50-2000), quality(0.2), code_quality(0.1)

### Math/Reasoning (35%)
- **Source**: `lighteval/mmlu`
- **Purpose**: Improve logical reasoning
- **Samples**: ~35,000
- **Filters**: length(20-1000), quality(0.2)

**Total**: ~95,000 training samples + 5% validation

---

## ğŸ“ Key Learnings

### Why Re-warmup?

When switching to new data distributions (code, math), the model can become unstable if we use the pretrained LR directly. Re-warming from LR/10 allows gentle adaptation.

### Why Replay?

Without replay, models suffer **catastrophic forgetting** - they lose general knowledge while learning specialized skills. 30% replay maintains a balance.

### Why Lower LR?

Mid-training refines an already trained model. High LR (5e-4) would destabilize learned representations. Lower LR (2e-4) makes fine adjustments.

---

## ğŸš€ Next Steps

With Task 1 complete, you can now:

1. **Test the pipeline:**
   ```bash
   # Pretrain â†’ Midtrain
   python trainer/train_pretrain.py ... # First
   python trainer/train_midtrain.py ... # Then
   ```

2. **Move to Task 2:** Create post-training configs (SFT, DPO, PPO)

3. **Experiment with mixtures:**
   - Try different replay ratios (20%, 40%)
   - Add more datasets
   - Adjust quality thresholds

---

## ğŸ› Troubleshooting

### Error: "Mid-training requires a pretrained checkpoint"

**Solution:** Always use `--from_weight`:
```bash
--from_weight pretrain # loads out/pretrain_512.pth # TODO: Verify this! 
```

### Issue: Loss increases instead of decreasing

**Possible causes:**
1. Learning rate too high â†’ Try `--learning_rate 1e-4`
2. Not enough warmup â†’ Try `--warmup_ratio 0.1`
3. Data quality issues â†’ Check filter thresholds

### Issue: "Failed to load HuggingFace dataset"

**Solution:** Check internet connection or try downloading manually:
```python
from datasets import load_dataset
dataset = load_dataset("iamtarun/python_code_instructions_18k_alpaca")
```

---

## ğŸ“ˆ Expected Results

After mid-training, the model should:
- âœ… Maintain general language ability (from replay)
- âœ… Show improved code understanding
- âœ… Better handle math/reasoning questions
- âœ… Loss should decrease smoothly (no spikes)

Compare before/after with:
```bash
python eval_llm.py --weight pretrain_512  # Before
python eval_llm.py --weight midtrain_512  # After
```

---

## ğŸ’¾ Artifacts Generated

After running mid-training, you'll have:

```
out/
â”œâ”€â”€ midtrain_512.pth              # Main model weights
â””â”€â”€ ...

checkpoints/
â”œâ”€â”€ midtrain_512_resume.pth       # Resume checkpoint
â””â”€â”€ ...

dataset/
â”œâ”€â”€ midtrain_phase1_default_train.jsonl  # Prepared training data
â””â”€â”€ midtrain_phase1_default_val.jsonl    # Prepared validation data
```

---

## âœ… Task 1 Summary

**What we built:**
- âœ… Mid-training config with 3 diverse HF datasets
- âœ… Code quality filter for specialized data
- âœ… Complete train_midtrain.py with LR re-warmup
- âœ… Validation and safety checks
- âœ… Comprehensive documentation

**Time spent:** ~2 hours (as estimated)

**Status:** READY FOR TASK 2 ğŸš€

---

**Questions or issues?** Check the commands above or refer to the code comments in `train_midtrain.py`.
